{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['article'][0]\n",
    "# len(dataset['test']['highlights'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following may not need to be used \n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text\n",
    "\n",
    "def load_data(path):\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    print('found {} files'.format(len(onlyfiles)))\n",
    "    all_text = []\n",
    "    for f in onlyfiles:\n",
    "        with open('{}/{}'.format(path, f)) as handle:\n",
    "            lines = clean_text(handle.readlines()[0])\n",
    "            all_text.append(lines)\n",
    "        \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(dataset['test']['article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_subsection(text):\n",
    "    for article in dataset['test']['article']:\n",
    "        word_count = 0\n",
    "        article_subsections = []\n",
    "        while len(article) > 512:\n",
    "            if len(article_subsections) == 0:\n",
    "                article_subsections.append(article[:512])\n",
    "                word_count = 512\n",
    "                article = article[:512]\n",
    "            if len(article) <= 412:\n",
    "                article_subsections.append(article[word_count-100:])\n",
    "            else:\n",
    "                article_subsections.append(article[word_count-100:word_count+412])   \n",
    "                word_count = word_count + 412\n",
    "                article = article[word_count:]\n",
    "        article_subsections.append(article)\n",
    "        article = article_subsections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, EncoderDecoderModel\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Data with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The tokenizer to be used to create embeddings \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints\n",
    "\n",
    "# forward\n",
    "input_ids = torch.tensor(tokenizer.encode(dataset['test']['article'], add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
    "\n",
    "# # training\n",
    "# outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n",
    "# loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "# # save and load from pretrained\n",
    "# model.save_pretrained(\"bert2bert\")\n",
    "# model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n",
    "\n",
    "# # generation\n",
    "# generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, NUM_OUT):\n",
    "        super(BERTClass, self).__init__()\n",
    "                   \n",
    "        self.l1 = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\",\"bert-base-uncased\")\n",
    "#         self.pre_classifier = torch.nn.Linear(768, 256)\n",
    "        self.classifier = torch.nn.Linear(768, NUM_OUT)\n",
    "#         self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "#         pooler = self.pre_classifier(pooler)\n",
    "#         pooler = torch.nn.Tanh()(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = []\n",
    "# model = BertClass(NUM_OUT)\n",
    "# model.to(device)\n",
    "# for i in range len(dataset['test']):\n",
    "#     # forward\n",
    "#     input_ids = torch.tensor(tokenizer.encode(dataset['test']['highlights'][i], add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "#     tokenized_dataset.append(model(input_ids=input_ids, decoder_input_ids=input_ids))\n",
    "\n",
    "# # training\n",
    "# outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n",
    "# loss, logits = outputs.loss, outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "clean_sum = []\n",
    "for text in tqdm(range(len(dataset['train']['article']))[:100]):\n",
    "    new = clean_text(dataset['train']['article'][text])\n",
    "    clean_data.append(new)\n",
    "    clean_sum.append(clean_text(dataset['train']['highlights'][text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = model(clean_data[0], ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for data in tqdm(clean_data):\n",
    "    try:\n",
    "        summary_text = summarizer(data, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "    except:\n",
    "        summary_text = summarizer(model(data), max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "    summary.append(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar(summary[2], clean_sum[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = 0\n",
    "for text_sum in range(len(summary)):\n",
    "    pred = summary[text_sum]\n",
    "    gold = clean_sum[text_sum]\n",
    "    score = similar(pred, gold)\n",
    "    print(\"PRED: {}\\nGOLD: {}\\nSCORE: {}\".format(pred, gold, score))\n",
    "    if score > .1:\n",
    "        good += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good / len(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bert Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
