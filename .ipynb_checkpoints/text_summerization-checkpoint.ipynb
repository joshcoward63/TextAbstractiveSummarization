{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sajia loading more data/combine with train data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"train\")\n",
    "val_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"validation\")\n",
    "test_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    dataset['test']['article'][i] = dataset['test']['article'][i].split()\n",
    "    dataset['test']['highlights'][i] = dataset['test']['highlights'][i].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following may not need to be used \n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text\n",
    "\n",
    "def load_data(path):\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    print('found {} files'.format(len(onlyfiles)))\n",
    "    all_text = []\n",
    "    for f in onlyfiles:\n",
    "        with open('{}/{}'.format(path, f)) as handle:\n",
    "            lines = clean_text(handle.readlines()[0])\n",
    "            all_text.append(lines)\n",
    "        \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_subsection(text):\n",
    "    for article in text:\n",
    "        word_count = 0\n",
    "        article_subsections = []\n",
    "        while len(article) > 512:\n",
    "            if len(article_subsections) == 0:\n",
    "                article_subsections.append(article[:512])\n",
    "                word_count = 512\n",
    "                article = article[412:]        \n",
    "            if len(article) > 412:\n",
    "#                 article_subsections.append(article[word_count-100:])\n",
    "#             else:\n",
    "                article_subsections.append(article[word_count-100:word_count+412])   \n",
    "                word_count = word_count + 412\n",
    "                article = article[word_count-100:]\n",
    "        article_subsections.append(article)\n",
    "       \n",
    "        article = article_subsections\n",
    "        print(article)\n",
    "        print(len(article))\n",
    "        print(article[1])\n",
    "        print(len(article[0]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_subsection(dataset['test']['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, EncoderDecoderModel\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Data with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The tokenizer to be used to create embeddings \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 128\n",
    "\n",
    "def convert_data_to_model_inputs(batch):\n",
    "    #Encodes the article\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length = encoder_max_length)\n",
    "    #Encodes the summary\n",
    "    outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length = decoder_max_length)\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size = batch_size,\n",
    "    remove_columns=[\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "val_data = val_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size = batch_size,\n",
    "    remove_columns = [\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns = [\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "test_data.set_format(\n",
    "    type=\"torch\", columns = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\",\"bert-base-uncased\")\n",
    "# set special tokens\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.min_length = 64\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.early_stopping = True\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_training_args.py\n",
    "!pip install git-python==1.0.3\n",
    "!pip install rouge_score\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from seq2seq_trainer import Seq2SeqTrainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqTrainingArguments(TrainingArguments):\n",
    "    label_smoothing: Optional[float] = field(default=0.0)\n",
    "    sortish_sampler: bool = field(default=False)\n",
    "    predict_with_generate: bool = field(default=False)        \n",
    "    adafactor: bool = field(default=False)\n",
    "    encoder_layerdrop: Optional[float] = field(default=None)\n",
    "    decoder_layerdrop: Optional[float] = field(default=None)\n",
    "    dropout: Optional[float] = field(default=None)\n",
    "    attention_dropout: Optional[float] = field(default=None)\n",
    "    lr_scheduler: Optional[str] = field(default=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "#     evaluate_during_training=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1000,\n",
    "    save_steps=500, \n",
    "    eval_steps=8000,\n",
    "    warmup_steps=2000,  \n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "#     fp16=True, \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(\"\")\n",
    "model.to(\"cuda\")\n",
    "batch_size = 64\n",
    "\n",
    "def evaluate_test_data(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length = encoder_max_length,return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"I like your take on facts vs opinions, it's certainly a logical stance to take. Sure you may have an opinion on a matter and have some reasonable justification for that opinion, but you should always remember to consider the facts against your opinion to ensure your decisions actually make sense because after all facts don't care about your opinion. In regards to your question I sort of have mixed feelings,certainly more often than not facts should overrule opinions, but sometimes in situations and scenarios you don't have all the necessary information and details to make decisions based on facts alone so you ultimately have to make an opinion based decision. For instance I personally won't be getting the COVID-19 vaccine at least not the first version or two of it. The reason being is because I have a severe peanut allergy and I am also seriously allergic to dairy products and some other things. While it may be a fact that the vaccine contains neither of those two substances, there have however been numerous cases of people who have similar allergies as I do who have in fact had serious reactions to the vaccine. While it's not a guarantee that this will happen to everyone who has allergies, therefore it's certainly not a fact that the virus will cause an allergic reaction in everyone who has allergies, yet I still will make the opinion based decision to not get the current version of the vaccine even if it's offered to me for free. In my opinion I much rather get the virus as I'm a healthy in shape adult then risk dying from a vaccine. Call me an idiot if you will but I'm not gonna risk it. Other than that, like Bruins suggested the only thing I can think of where facts are disregarded over opinions would be religion. No where else will you find a bunch of psychos blinding following  some random idea then at a church or at a psych ward with a bunch of schizophrenics.\"\n",
    "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader for quick model comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "clean_sum = []\n",
    "for text in tqdm(range(len(dataset['train']['article']))[:1]):\n",
    "    new = clean_text(dataset['train']['article'][text])\n",
    "    clean_data.append(new)\n",
    "    clean_sum.append(clean_text(dataset['train']['highlights'][text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = [\"google/pegasus-cnn_dailymail\", \"t5-base\", \"sshleifer/distilbart-cnn-12-6\", \"facebook/bart-large-cnn\",\"nsi319/legal-led-base-16384\", \"google/pegasus-newsroom\", \"google/pegasus-wikihow\", \"ml6team/mt5-small-german-finetune-mlsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarizers = {}\n",
    "for name in tqdm(sum_list):\n",
    "    summarizers[name] = pipeline(\"summarization\", model=name, tokenizer=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_preds = {}\n",
    "for m in tqdm(summarizers):\n",
    "    summary = []\n",
    "    for data in tqdm(range(len(clean_data))):\n",
    "        try:\n",
    "            summary_text = summarizers[m](clean_data[data], clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "        except:\n",
    "            summary_text = summarizers[m](model(clean_data[data]), clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "        summary.append(summary_text)\n",
    "    sum_preds[m] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_scores = {}\n",
    "for model_name in tqdm(sum_preds):\n",
    "    good = 0\n",
    "    for text_sum in range(len(summary)):\n",
    "        pred = sum_preds[model_name][text_sum]\n",
    "        gold = clean_sum[text_sum]\n",
    "        score = similar(pred, gold)\n",
    "        if score > .1:\n",
    "            good += 1\n",
    "    sum_scores[model_name] = good / len(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bert Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
