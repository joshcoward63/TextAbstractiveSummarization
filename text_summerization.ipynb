{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "## Josh Coward, Ryan Pacheco, Sajia Zafreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, EncoderDecoderModel\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import datasets\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin here if you wish to run the fine tuned BERT model. Otherwise skip to `Pre Trained Comp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"train\")\n",
    "dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\")\n",
    "val_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"validation\")\n",
    "test_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    dataset['test']['article'][i] = dataset['test']['article'][i].split()\n",
    "    dataset['test']['highlights'][i] = dataset['test']['highlights'][i].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following may not need to be used \n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text\n",
    "\n",
    "def load_data(path):\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    print('found {} files'.format(len(onlyfiles)))\n",
    "    all_text = []\n",
    "    for f in onlyfiles:\n",
    "        with open('{}/{}'.format(path, f)) as handle:\n",
    "            lines = clean_text(handle.readlines()[0])\n",
    "            all_text.append(lines)\n",
    "        \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_subsection(text):\n",
    "    for article in text:\n",
    "        word_count = 0\n",
    "        article_subsections = []\n",
    "        while len(article) > 512:\n",
    "            if len(article_subsections) == 0:\n",
    "                article_subsections.append(article[:512])\n",
    "                word_count = 512\n",
    "                article = article[412:]        \n",
    "            if len(article) > 412:\n",
    "#                 article_subsections.append(article[word_count-100:])\n",
    "#             else:\n",
    "                article_subsections.append(article[word_count-100:word_count+412])   \n",
    "                word_count = word_count + 412\n",
    "                article = article[word_count-100:]\n",
    "        article_subsections.append(article)\n",
    "       \n",
    "        article = article_subsections\n",
    "        print(article)\n",
    "        print(len(article))\n",
    "        print(article[1])\n",
    "        print(len(article[0]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_subsection(dataset['test']['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Data with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The tokenizer to be used to create embeddings \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 128\n",
    "\n",
    "def convert_data_to_model_inputs(batch):\n",
    "    #Encodes the article\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length = encoder_max_length)\n",
    "    #Encodes the summary\n",
    "    outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length = decoder_max_length)\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size = batch_size,\n",
    "    remove_columns=[\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "val_data = val_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size = batch_size,\n",
    "    remove_columns = [\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns = [\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "test_data.set_format(\n",
    "    type=\"torch\", columns = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\",\"bert-base-uncased\")\n",
    "# set special tokens\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.min_length = 64\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.early_stopping = True\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_training_args.py\n",
    "!pip install git-python==1.0.3\n",
    "!pip install rouge_score\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from seq2seq_trainer import Seq2SeqTrainer\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqTrainingArguments(TrainingArguments):\n",
    "    label_smoothing: Optional[float] = field(default=0.0)\n",
    "    sortish_sampler: bool = field(default=False)\n",
    "    predict_with_generate: bool = field(default=False)        \n",
    "    adafactor: bool = field(default=False)\n",
    "    encoder_layerdrop: Optional[float] = field(default=None)\n",
    "    decoder_layerdrop: Optional[float] = field(default=None)\n",
    "    dropout: Optional[float] = field(default=None)\n",
    "    attention_dropout: Optional[float] = field(default=None)\n",
    "    lr_scheduler: Optional[str] = field(default=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "#     evaluate_during_training=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1000,\n",
    "    save_steps=500, \n",
    "    eval_steps=8000,\n",
    "    warmup_steps=2000,  \n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "#     fp16=True, \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(\"\")\n",
    "model.to(\"cuda\")\n",
    "batch_size = 64\n",
    "\n",
    "def evaluate_test_data(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length = encoder_max_length,return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"I like your take on facts vs opinions, it's certainly a logical stance to take. Sure you may have an opinion on a matter and have some reasonable justification for that opinion, but you should always remember to consider the facts against your opinion to ensure your decisions actually make sense because after all facts don't care about your opinion. In regards to your question I sort of have mixed feelings,certainly more often than not facts should overrule opinions, but sometimes in situations and scenarios you don't have all the necessary information and details to make decisions based on facts alone so you ultimately have to make an opinion based decision. For instance I personally won't be getting the COVID-19 vaccine at least not the first version or two of it. The reason being is because I have a severe peanut allergy and I am also seriously allergic to dairy products and some other things. While it may be a fact that the vaccine contains neither of those two substances, there have however been numerous cases of people who have similar allergies as I do who have in fact had serious reactions to the vaccine. While it's not a guarantee that this will happen to everyone who has allergies, therefore it's certainly not a fact that the virus will cause an allergic reaction in everyone who has allergies, yet I still will make the opinion based decision to not get the current version of the vaccine even if it's offered to me for free. In my opinion I much rather get the virus as I'm a healthy in shape adult then risk dying from a vaccine. Call me an idiot if you will but I'm not gonna risk it. Other than that, like Bruins suggested the only thing I can think of where facts are disregarded over opinions would be religion. No where else will you find a bunch of psychos blinding following  some random idea then at a church or at a psych ward with a bunch of schizophrenics.\"\n",
    "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin here if you wish to just do Pre Trained model comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load Datasets here\n",
    "* https://huggingface.co/datasets?filter=task_ids:summarization,languages:en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bill = load_dataset(\"billsum\")\n",
    "dataset_cnn = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "dataset_sam = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the number of articles you wish to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_cnn = []\n",
    "clean_sum_cnn = []\n",
    "for text in tqdm(range(len(dataset_cnn['test']['article']))[:200]):\n",
    "    new = clean_text(dataset_cnn['test']['article'][text])\n",
    "    clean_data_cnn.append(new)\n",
    "    clean_sum_cnn.append(clean_text(dataset_cnn['test']['highlights'][text]))\n",
    "    \n",
    "clean_data_bill = []\n",
    "clean_sum_bill = []\n",
    "for text in tqdm(range(len(dataset_bill['test']['text']))[:200]):\n",
    "    new = clean_text(dataset_bill['test']['text'][text])\n",
    "    clean_data_bill.append(new)\n",
    "    clean_sum_bill.append(clean_text(dataset_bill['test']['summary'][text]))\n",
    "    \n",
    "clean_data_sam = []\n",
    "clean_sum_sam = []\n",
    "for text in tqdm(range(len(dataset_sam['test']['dialogue']))[:200]):\n",
    "    new = clean_text(dataset_sam['test']['dialogue'][text])\n",
    "    clean_data_sam.append(new)\n",
    "    clean_sum_sam.append(clean_text(dataset_sam['test']['summary'][text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more models to this list if you wish to add them to the comparision\n",
    "* https://huggingface.co/models?pipeline_tag=summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = [\"google/pegasus-cnn_dailymail\", \"t5-base\", \"sshleifer/distilbart-cnn-12-6\", \"facebook/bart-large-cnn\",\"nsi319/legal-led-base-16384\", \"google/pegasus-newsroom\", \"google/pegasus-wikihow\", \"ml6team/mt5-small-german-finetune-mlsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarizers = {}\n",
    "for name in tqdm(sum_list):\n",
    "    summarizers[name] = pipeline(\"summarization\", model=name, tokenizer=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_models(clean_data, clean_sum, summarizers, model):\n",
    "    sum_preds = {}\n",
    "    for m in tqdm(summarizers):\n",
    "        summary = []\n",
    "        for data in tqdm(range(len(clean_data))):\n",
    "            try:\n",
    "                summary_text = summarizers[m](clean_data[data], clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except:\n",
    "                summary_text = summarizers[m](model(clean_data[data]), clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "            summary.append(summary_text)\n",
    "        sum_preds[m] = summary\n",
    "\n",
    "    sum_scores = {}\n",
    "    for model_name in tqdm(sum_preds):\n",
    "        sum_scores[model_name] = {}\n",
    "        good_score = 0\n",
    "        pred = []\n",
    "        gold = []\n",
    "        for text_sum in range(len(sum_preds[model_name])):\n",
    "            pred.append(sum_preds[model_name][text_sum])\n",
    "            gold.append(clean_sum[text_sum])\n",
    "            score = similar(sum_preds[model_name][text_sum], clean_sum[text_sum])\n",
    "            if score > .1:\n",
    "                good_score += 1\n",
    "        try:\n",
    "            good = rouge.compute(predictions=pred, references=gold, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        except:\n",
    "            good = [0.0, 0.0, 0.0] \n",
    "        sum_scores[model_name]['rouge'] = good\n",
    "        sum_scores[model_name]['similar'] = good_score / len(summary)\n",
    "\n",
    "\n",
    "    new_scores = {}\n",
    "    for model in sum_scores:\n",
    "        new_scores[model] = {}\n",
    "        new_scores[model]['precision'] = sum_scores[model]['rouge'][0]\n",
    "        new_scores[model]['recall'] = sum_scores[model]['rouge'][1]\n",
    "        new_scores[model]['fmeasure'] = sum_scores[model]['rouge'][2]\n",
    "        new_scores[model]['similar'] = sum_scores[model]['similar']\n",
    "    return new_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call summarize_modes() to summarize all the datasets that have been cleaned through all the models in the summarizers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_scores = summarize_models(clean_data_cnn, clean_sum_cnn, summarizers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bill_scores = summarize_models(clean_data_bill, clean_sum_bill, summarizers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_scores = summarize_models(clean_data_sam, clean_sum_sam, summarizers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Precision = $\\frac{TP}{TP + FP}$\n",
    "* Recall = $\\frac{TP}{TP + FN}$\n",
    "* Fmeasure = 2 * $\\frac{Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### If more models have been added copy the next two cells to show results, replacing `cnn_scores` with `{VALUE RETURNED FROM summarize_models()}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in cnn_scores:\n",
    "    print(\"{}:\\n\\tPrecision: {}\\n\\tRecall: {}\\n\\tFmeasure: {}\\n\\tSimilar: {}\\n\\t\".format(score, cnn_scores[score]['precision'],cnn_scores[score]['recall'],cnn_scores[score]['fmeasure'],cnn_scores[score]['similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cnn_scores).plot(kind='bar', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in bill_scores:\n",
    "    print(\"{}:\\n\\tPrecision: {}\\n\\tRecall: {}\\n\\tFmeasure: {}\\n\\tSimilar: {}\\n\\t\".format(score, bill_scores[score]['precision'],bill_scores[score]['recall'],bill_scores[score]['fmeasure'],bill_scores[score]['similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bill_scores).plot(kind='bar', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in sam_scores:\n",
    "    print(\"{}:\\n\\tPrecision: {}\\n\\tRecall: {}\\n\\tFmeasure: {}\\n\\tSimilar: {}\\n\\t\".format(score, sam_scores[score]['precision'],sam_scores[score]['recall'],sam_scores[score]['fmeasure'],sam_scores[score]['similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sam_scores).plot(kind='bar', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
