{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['article'][0]\n",
    "# len(dataset['test']['highlights'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following may not need to be used \n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text\n",
    "\n",
    "def load_data(path):\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    print('found {} files'.format(len(onlyfiles)))\n",
    "    all_text = []\n",
    "    for f in onlyfiles:\n",
    "        with open('{}/{}'.format(path, f)) as handle:\n",
    "            lines = clean_text(handle.readlines()[0])\n",
    "            all_text.append(lines)\n",
    "        \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(dataset['test']['article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_subsection(text):\n",
    "    for article in text:\n",
    "        article = clean_text(article)\n",
    "        word_count = 0\n",
    "        article_subsections = []\n",
    "        while len(article) > 512:\n",
    "            if len(article_subsections) == 0:\n",
    "                article_subsections.append(article[:512])\n",
    "                word_count = 512\n",
    "                article = article[512:]\n",
    "            if len(article) <= 412:\n",
    "                article_subsections.append(article[word_count-100:])\n",
    "            else:\n",
    "                article_subsections.append(article[word_count-100:word_count+412])   \n",
    "                word_count = word_count + 412\n",
    "                article = article[word_count:]\n",
    "        article_subsections.append(article)\n",
    "        article = article_subsections\n",
    "        print(article)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_subsection(dataset['test']['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, EncoderDecoderModel\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Data with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The tokenizer to be used to create embeddings \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints\n",
    "\n",
    "# forward\n",
    "input_ids = torch.tensor(tokenizer.encode(dataset['test']['article'], add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
    "\n",
    "# # training\n",
    "# outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n",
    "# loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "# # save and load from pretrained\n",
    "# model.save_pretrained(\"bert2bert\")\n",
    "# model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n",
    "\n",
    "# # generation\n",
    "# generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, NUM_OUT):\n",
    "        super(BERTClass, self).__init__()\n",
    "                   \n",
    "        self.l1 = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\",\"bert-base-uncased\")\n",
    "#         self.pre_classifier = torch.nn.Linear(768, 256)\n",
    "        self.classifier = torch.nn.Linear(768, NUM_OUT)\n",
    "#         self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "#         pooler = self.pre_classifier(pooler)\n",
    "#         pooler = torch.nn.Tanh()(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = []\n",
    "# model = BertClass(NUM_OUT)\n",
    "# model.to(device)\n",
    "# for i in range len(dataset['test']):\n",
    "#     # forward\n",
    "#     input_ids = torch.tensor(tokenizer.encode(dataset['test']['highlights'][i], add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "#     tokenized_dataset.append(model(input_ids=input_ids, decoder_input_ids=input_ids))\n",
    "\n",
    "# # training\n",
    "# outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n",
    "# loss, logits = outputs.loss, outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "clean_sum = []\n",
    "for text in tqdm(range(len(dataset['train']['article']))[:1]):\n",
    "    new = clean_text(dataset['train']['article'][text])\n",
    "    clean_data.append(new)\n",
    "    clean_sum.append(clean_text(dataset['train']['highlights'][text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = [\"google/pegasus-cnn_dailymail\", \"t5-base\", \"sshleifer/distilbart-cnn-12-6\", \"facebook/bart-large-cnn\",\"nsi319/legal-led-base-16384\", \"google/pegasus-newsroom\", \"google/pegasus-wikihow\", \"ml6team/mt5-small-german-finetune-mlsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarizers = {}\n",
    "for name in tqdm(sum_list):\n",
    "    summarizers[name] = pipeline(\"summarization\", model=name, tokenizer=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_preds = {}\n",
    "for m in tqdm(summarizers):\n",
    "    summary = []\n",
    "    for data in tqdm(range(len(clean_data))):\n",
    "        try:\n",
    "            summary_text = summarizers[m](clean_data[data], clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "        except:\n",
    "            summary_text = summarizers[m](model(clean_data[data]), clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "        summary.append(summary_text)\n",
    "    sum_preds[m] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_scores = {}\n",
    "for model_name in tqdm(sum_preds):\n",
    "    good = 0\n",
    "    for text_sum in range(len(summary)):\n",
    "        pred = sum_preds[model_name][text_sum]\n",
    "        gold = clean_sum[text_sum]\n",
    "        score = similar(pred, gold)\n",
    "        if score > .1:\n",
    "            good += 1\n",
    "    sum_scores[model_name] = good / len(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bert Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
