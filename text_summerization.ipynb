{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "## Josh Coward, Ryan Pacheco, Sajia Zafreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, EncoderDecoderModel\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import datasets\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin here if you wish to run the fine tuned BERT model. Otherwise skip to `Pre Trained Comp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"train\")\n",
    "dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\")\n",
    "val_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"validation\")\n",
    "test_data = load_dataset(\"cnn_dailymail\",\"3.0.0\",split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    return text\n",
    "\n",
    "def load_data(path):\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    print('found {} files'.format(len(onlyfiles)))\n",
    "    all_text = []\n",
    "    for f in onlyfiles:\n",
    "        with open('{}/{}'.format(path, f)) as handle:\n",
    "            lines = clean_text(handle.readlines()[0])\n",
    "            all_text.append(lines)\n",
    "        \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function iterates through all the articles given and creates a list of overlapping sections for each article \n",
    "# if that article is greater than 512 words  \n",
    "def overlapping_subsection(dataset):\n",
    "    articles = []\n",
    "    for article in tqdm(dataset):\n",
    "        article = article.split()\n",
    "        word_count = 0\n",
    "        article_subsections = []\n",
    "        while len(article) > 512:\n",
    "            if len(article_subsections) == 0:\n",
    "                article_subsections.append(' '.join(word for word in article[:512]))\n",
    "                word_count = 512\n",
    "                article = article[word_count-100:]       \n",
    "            if len(article) > 412:\n",
    "                article_subsections.append(' '.join(word for word in article[:512]))   \n",
    "                word_count = word_count + 412\n",
    "                article = article[412:]\n",
    "            if len(article) < 412:\n",
    "                article_subsections.append(' '.join(word for word in article))   \n",
    "        if len(article_subsections) != 0: \n",
    "            article = article_subsections\n",
    "        articles.append(article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the orginal data after running it through the overlapping function saved as a pandas DataFrame\n",
    "train_df =  pd.DataFrame({'article':overlapping_subsection(train_data['article']),'highlights':train_data['highlights'],\"id\": train_data['id']})\n",
    "test_df = pd.DataFrame({'article':overlapping_subsection(test_data['article']),'highlights':test_data['highlights'],\"id\": test_data['id']})\n",
    "val_df =  pd.DataFrame({'article':overlapping_subsection(val_data['article']),'highlights':val_data['highlights'],\"id\": val_data['id']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokenizer using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The tokenizer to be used to create embeddings for both Articles and Summaries\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # Change batch size to 4 for faster but less accurate training\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 128\n",
    "\n",
    "def convert_data_to_model_inputs(batch):\n",
    "    #Encodes the article\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length = encoder_max_length)\n",
    "    #Encodes the summary\n",
    "    outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length = decoder_max_length)\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: If data has already been mapped and saved to file DO NOT run this cell\n",
    "\n",
    "# Uncomment out the following two line two train on a small subset\n",
    "# train_data = train_data.select(range(32))\n",
    "# val_data = val_data.select(range(32))\n",
    "\n",
    "train_data = train_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size = batch_size,\n",
    "    remove_columns=[\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "val_data = val_data.map(\n",
    "    convert_data_to_model_inputs,\n",
    "    batched = True,\n",
    "    batch_size = batch_size,\n",
    "    remove_columns = [\"article\",\"highlights\", \"id\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves Mapped Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following saves the training and validation data to file\n",
    "# File size < 1.75 Gb\n",
    "with open('train_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('val_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Mapped Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following loads pre-mapped train/validation data\n",
    "with open('train_data.pickle', 'rb') as handle:\n",
    "     train_data = pickle.load(handle)\n",
    "        \n",
    "with open('val_data.pickle', 'rb') as handle:\n",
    "    val_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the base encoder decoder model\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\",\"bert-base-uncased\")\n",
    "\n",
    "# set model configuration\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.min_length = 64\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.early_stopping = True\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output1 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output1.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output1.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output1.fmeasure, 4),\n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frees up unused memory \n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='steps',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1000,\n",
    "    save_steps=500, \n",
    "    eval_steps=8000,\n",
    "    warmup_steps=2000,  \n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=True, #Comment out this line if training a non-CUDA device\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(\"checkpoint-500\")\n",
    "model.to(\"cuda\") # Comment out this line if not using CUDA\n",
    "batch_size = 64\n",
    "\n",
    "def evaluate_test_data(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length = encoder_max_length,return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\") # Remove \".to(\"cuda\")\" if not using CUDA\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\") # Remove \".to(\"cuda\")\" if not using CUDA\n",
    "    \n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "results = test_data.map(evaluate_test_data, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "\n",
    "rouge_output1 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output1)\n",
    "print(rouge_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Our proposal looks into the reasonability, feasibility, and benefits of introducing plant-based dining options to the Boise State community. Currently, Boise State University lacks a vegan exclusive dining option on campus. This creates a variety of problems not only at the University, but at an ecological scale. With a current campus population of 22,064, there are an array of dietary restrictions, allergens, and lifestyles. Adding a plant based restaurant could help appease those students and also offer a healthier dining option. Nutrition is crucial for students and their success with studying, work ethic, and attendance. Extensive nutritional research says that plant based foods can directly affect mental capacity among school-aged children. As an example, iron deficiency can decrease dopamine transmission, thus negatively impacting a students cognition. A balanced, plant based diet can also induce better learning behaviors and learning environments. Lastly, researchers have found that plant based foods have impacted student’s exam scores and more positive school related outcomes. This also creates a convenience for students at BSU, who otherwise would search for food options off-campus. We believe there is extreme significance in this proposal that most others are unaware of. By introducing vegan options, there would be an improvement in student and faculty health for those customers. In an omnivorous diet, there are many factors that are destructive to our health. Heart disease, cancer, diabetes, obesity and strokes are just some of the few. With an ever changing environment that is rapidly increasing in warmth, humans are front and center for the cause of another imminent, mass extinction. Waste from animals whether nitrous oxide, ammonia, methane, or feces & urine, the waste amasses a total weight of 7,742,000,000 (or 7.742 billion) pounds. While the gases emitted attributes to air pollution. Research suggests that roughly 80% of ammonia emissions in the U.S comes from animal waste and it’s estimated that over 50 percent of world greenhouse gases. More than a third of all raw materials and fossil fuels consumed in the U.S. are used in animal agriculture. Researchers also state that animal agriculture is responsible for 9 percent of global carbon dioxide emissions, 35 percent to 40 percent of global methane emissions, and 65 percent of nitrous oxide emissions. These pollutants affect natural environments such as the amazon rainforest, the great barrier reef, and even places in our own backyard such as the Boise greenbelt. Not only would vegan dining options help combat climate change or health related illnesses, it would help combat future pandemics and animal cruelty. As COVID-19 continues to roar through the United States taking the lives of many, the source of the virus has been in question by conspirists yet, scientists have stated that it likely came from a wet market in Wuhan, China. Wet markets are similar to the likes of factory farms that house billions of animals worldwide annually. These compact and dirty environments that animals are bred in, and exploited, have been the source of three-fourths of the most recent pandemics in history. Diseases like SARS, HIV, COVID, Swine flu and bird flu all originated from animals being in close contact with humans. World renown doctors have compiled research and substantial evidence that if we do not change our food system, and work towards a plant based system, pandemics will become inevitable. Furthermore, authors behind a new doctor-backed white paper say they have tied most, if not all of the worlds most major outbreaks to animal exploitation since 1900. The evidence is damning, and if we do not begin working towards a plant based future, the feasibility of humankind attending universities, public gatherings, or being in physical contact with one another, becomes bleak. \"\n",
    "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin here if you wish to just do Pre Trained model comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load Datasets here\n",
    "* https://huggingface.co/datasets?filter=task_ids:summarization,languages:en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bill = load_dataset(\"billsum\")\n",
    "dataset_cnn = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "dataset_sam = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the number of articles you wish to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_cnn = []\n",
    "clean_sum_cnn = []\n",
    "for text in tqdm(range(len(dataset_cnn['test']['article']))[:200]):\n",
    "    new = clean_text(dataset_cnn['test']['article'][text])\n",
    "    clean_data_cnn.append(new)\n",
    "    clean_sum_cnn.append(clean_text(dataset_cnn['test']['highlights'][text]))\n",
    "    \n",
    "clean_data_bill = []\n",
    "clean_sum_bill = []\n",
    "for text in tqdm(range(len(dataset_bill['test']['text']))[:200]):\n",
    "    new = clean_text(dataset_bill['test']['text'][text])\n",
    "    clean_data_bill.append(new)\n",
    "    clean_sum_bill.append(clean_text(dataset_bill['test']['summary'][text]))\n",
    "    \n",
    "clean_data_sam = []\n",
    "clean_sum_sam = []\n",
    "for text in tqdm(range(len(dataset_sam['test']['dialogue']))[:200]):\n",
    "    new = clean_text(dataset_sam['test']['dialogue'][text])\n",
    "    clean_data_sam.append(new)\n",
    "    clean_sum_sam.append(clean_text(dataset_sam['test']['summary'][text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more models to this list if you wish to add them to the comparision\n",
    "* https://huggingface.co/models?pipeline_tag=summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = [\"google/pegasus-cnn_dailymail\", \"t5-base\", \"sshleifer/distilbart-cnn-12-6\", \"facebook/bart-large-cnn\",\"nsi319/legal-led-base-16384\", \"google/pegasus-newsroom\", \"google/pegasus-wikihow\", \"ml6team/mt5-small-german-finetune-mlsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarizers = {}\n",
    "for name in tqdm(sum_list):\n",
    "    summarizers[name] = pipeline(\"summarization\", model=name, tokenizer=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_models(clean_data, clean_sum, summarizers, model):\n",
    "    sum_preds = {}\n",
    "    for m in tqdm(summarizers):\n",
    "        summary = []\n",
    "        for data in tqdm(range(len(clean_data))):\n",
    "            try:\n",
    "                summary_text = summarizers[m](clean_data[data], clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except:\n",
    "                summary_text = summarizers[m](model(clean_data[data]), clean_sum[data], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "            summary.append(summary_text)\n",
    "        sum_preds[m] = summary\n",
    "\n",
    "    sum_scores = {}\n",
    "    for model_name in tqdm(sum_preds):\n",
    "        sum_scores[model_name] = {}\n",
    "        good_score = 0\n",
    "        pred = []\n",
    "        gold = []\n",
    "        for text_sum in range(len(sum_preds[model_name])):\n",
    "            pred.append(sum_preds[model_name][text_sum])\n",
    "            gold.append(clean_sum[text_sum])\n",
    "            score = similar(sum_preds[model_name][text_sum], clean_sum[text_sum])\n",
    "            if score > .1:\n",
    "                good_score += 1\n",
    "        try:\n",
    "            good = rouge.compute(predictions=pred, references=gold, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        except:\n",
    "            good = [0.0, 0.0, 0.0] \n",
    "        sum_scores[model_name]['rouge'] = good\n",
    "        sum_scores[model_name]['similar'] = good_score / len(summary)\n",
    "\n",
    "\n",
    "    new_scores = {}\n",
    "    for model in sum_scores:\n",
    "        new_scores[model] = {}\n",
    "        new_scores[model]['precision'] = sum_scores[model]['rouge'][0]\n",
    "        new_scores[model]['recall'] = sum_scores[model]['rouge'][1]\n",
    "        new_scores[model]['fmeasure'] = sum_scores[model]['rouge'][2]\n",
    "        new_scores[model]['similar'] = sum_scores[model]['similar']\n",
    "    return new_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call summarize_modes() to summarize all the datasets that have been cleaned through all the models in the summarizers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_scores = summarize_models(clean_data_cnn, clean_sum_cnn, summarizers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bill_scores = summarize_models(clean_data_bill, clean_sum_bill, summarizers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_scores = summarize_models(clean_data_sam, clean_sum_sam, summarizers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Precision = $\\frac{TP}{TP + FP}$\n",
    "* Recall = $\\frac{TP}{TP + FN}$\n",
    "* Fmeasure = 2 * $\\frac{Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### If more models have been added copy the next two cells to show results, replacing `cnn_scores` with `{VALUE RETURNED FROM summarize_models()}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in cnn_scores:\n",
    "    print(\"{}:\\n\\tPrecision: {}\\n\\tRecall: {}\\n\\tFmeasure: {}\\n\\tSimilar: {}\\n\\t\".format(score, cnn_scores[score]['precision'],cnn_scores[score]['recall'],cnn_scores[score]['fmeasure'],cnn_scores[score]['similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cnn_scores).plot(kind='bar', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in bill_scores:\n",
    "    print(\"{}:\\n\\tPrecision: {}\\n\\tRecall: {}\\n\\tFmeasure: {}\\n\\tSimilar: {}\\n\\t\".format(score, bill_scores[score]['precision'],bill_scores[score]['recall'],bill_scores[score]['fmeasure'],bill_scores[score]['similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bill_scores).plot(kind='bar', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in sam_scores:\n",
    "    print(\"{}:\\n\\tPrecision: {}\\n\\tRecall: {}\\n\\tFmeasure: {}\\n\\tSimilar: {}\\n\\t\".format(score, sam_scores[score]['precision'],sam_scores[score]['recall'],sam_scores[score]['fmeasure'],sam_scores[score]['similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sam_scores).plot(kind='bar', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
